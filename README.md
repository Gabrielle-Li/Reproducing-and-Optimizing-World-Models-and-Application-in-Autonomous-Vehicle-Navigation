# Reproducing-and-Optimizing-World-Models-and-Application-in-Autonomous-Vehicle-Navigation

In deep learning, the core concept of a world model is to enable a model to learn and construct an internal simulation environment based on observations of the current environment. By simulating this environment, the model can predict future states, thereby aiding in decision-making and the generation of new behavioral strategies.
The world model comprises three key components: the vision module, the memory model, and the controller. The vision module uses a variational autoencoder (VAE) to encode environmental observations into a low-dimensional latent vector, effectively compressing observation images into concise and specific representations. The memory model employs recurrent neural networks (RNNs) to model and predict changes in the environment by leveraging historical information stored in the RNN's hidden layers. It predicts future states based on the hidden state and the agent's actions. The controller utilizes reinforcement learning to decide on actions based on the environment and historical memory, interacting with the environment and promoting iterative training. When the vision module generates the latent feature vector of the input image through the encoder, the memory model predicts the next state based on the current input. The current state input, together with the hidden state of the memory module, is provided to the controller as input for reinforcement learning. The controller then outputs an action, which alters the environment into a new state, initiating a new iteration.

In the first step, we reproduced and optimized the world model based on 'CarRacing-v2', masking two regions per frame with sizes of 5×5, 10×10, or 15×15 pixels. The encoder extracted features from these occluded frames, and the decoder aimed to reconstruct the images while regressing the occlusion centers. We enhanced the memory module by adding trainable weights to its hidden layers, allowing the decision module to focus on more effective memory information. This led to more precise decisions and higher scores, making the world model more robust and efficient in learning and decision-making within the simulated environment. Then, we applied this optimized model to autonomous driving, focusing on trajectory planning to find the fastest route amid random traffic congestion and accidents. We designed a 2D map with two lanes to enable dynamic route adjustments. An ISAC signal captured sensing data, which was transformed into bird's-eye view images for input. This simplification contributed to more accurate and low-latency planning while optimizing spectrum resource use, benefiting vehicle communication networks. By inputting both the global map and sequential bird's-eye view images, the world model effectively planned subsequent moves and optimized the global route, addressing the dynamic trajectory planning challenges faced by autonomous vehicles.
